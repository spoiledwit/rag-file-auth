{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12640545,"sourceType":"datasetVersion","datasetId":7987881},{"sourceType":"datasetVersion","sourceId":12641266,"datasetId":7988291}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ad9856d8","cell_type":"code","source":"# Kaggle-optimized installation\n!pip install pinecone sentence-transformers rank-bm25 langchain\n# Most other packages are pre-installed on Kaggle","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:03:38.268446Z","iopub.execute_input":"2025-08-01T13:03:38.269076Z","iopub.status.idle":"2025-08-01T13:05:02.447954Z","shell.execute_reply.started":"2025-08-01T13:03:38.269048Z","shell.execute_reply":"2025-08-01T13:05:02.447117Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pinecone\n  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting rank-bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.6.15)\nCollecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.14.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.5.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (1.26.4)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.4)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank-bm25) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank-bm25) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank-bm25) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank-bm25) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank-bm25) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank-bm25) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\nDownloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pinecone-plugin-interface, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pinecone-plugin-assistant, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pinecone, nvidia-cusolver-cu12, rank-bm25\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 rank-bm25-0.2.2\n","output_type":"stream"}],"execution_count":3},{"id":"a0e82941","cell_type":"code","source":"import os\nimport logging\nimport json\nimport time\nfrom typing import List, Dict, Optional, Tuple, Union\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# Core packages\nfrom sentence_transformers import SentenceTransformer\n\n# Hugging Face for text generation\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# NLTK for simple tokenization\nimport nltk\ntry:\n    nltk.download('punkt', quiet=True)\nexcept:\n    pass\n\n# LangChain for text splitting\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\n\n# BM25 for keyword search\nfrom rank_bm25 import BM25Okapi\n\n# Pinecone (you can also replace this with FAISS for fully local)\nfrom pinecone import Pinecone, ServerlessSpec\n\n# Evaluation\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nprint(\"✅ All libraries imported (Hugging Face only)!\")","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:05:07.274662Z","iopub.execute_input":"2025-08-01T13:05:07.275449Z","iopub.status.idle":"2025-08-01T13:05:07.282116Z","shell.execute_reply.started":"2025-08-01T13:05:07.275423Z","shell.execute_reply":"2025-08-01T13:05:07.281242Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ All libraries imported (Hugging Face only)!\n","output_type":"stream"}],"execution_count":5},{"id":"ce42f865","cell_type":"code","source":"# Configuration\nCONFIG = {\n    \"embedding_model\": \"BAAI/bge-small-en\",\n    \"generation_model\": \"Qwen/Qwen2.5-3B-Instruct\",  # Your preference\n    \n    \"index_name\": \"hybrid-rag-langchain\",\n    \n    # LangChain RecursiveCharacterTextSplitter parameters\n    \"chunk_size\": 300,       # characters per chunk\n    \"chunk_overlap\": 50,     # overlapping characters (20% overlap)\n    \"separators\": [\"\\n\\n\", \"\\n\", \" \", \"\"],  # Hierarchy of separators\n    \"retrieval_k\": 10\n}\n\n# Set your Pinecone API key (only one API key needed now!)\nPINECONE_API_KEY = \"pcsk_6m2PRg_1qqfqLoS7ZEfXyacwJzrjwkKaQUA5aW3VQjV7wVoMfLH7S8MYZPG2sD5QaVeSE\"\n\n# Check if GPU is available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"🔧 Using device: {device}\")\n\n# Verify API key\nif not PINECONE_API_KEY or PINECONE_API_KEY == \"your_pinecone_api_key_here\":\n    print(\"⚠️ Please set your PINECONE_API_KEY\")\nelse:\n    print(\"✅ Pinecone API key configured\")\n\nprint(f\"🎯 Models selected:\")\nprint(f\"  Embedding: {CONFIG['embedding_model']}\")\nprint(f\"  Generation: {CONFIG['generation_model']}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:05:14.285022Z","iopub.execute_input":"2025-08-01T13:05:14.285344Z","iopub.status.idle":"2025-08-01T13:05:14.291923Z","shell.execute_reply.started":"2025-08-01T13:05:14.285319Z","shell.execute_reply":"2025-08-01T13:05:14.290992Z"},"trusted":true},"outputs":[{"name":"stdout","text":"🔧 Using device: cuda\n✅ Pinecone API key configured\n🎯 Models selected:\n  Embedding: BAAI/bge-small-en\n  Generation: Qwen/Qwen2.5-3B-Instruct\n","output_type":"stream"}],"execution_count":7},{"id":"254889f2","cell_type":"code","source":"# Initialize embedding model\nprint(f\"🔄 Loading embedding model: {CONFIG['embedding_model']}\")\nembedding_model = SentenceTransformer(CONFIG['embedding_model'])\nembedding_dimension = embedding_model.get_sentence_embedding_dimension()\n\n# Initialize LangChain text splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CONFIG['chunk_size'],\n    chunk_overlap=CONFIG['chunk_overlap'],\n    separators=CONFIG['separators'],\n    length_function=len,  # Use character count\n    is_separator_regex=False\n)\n\nprint(f\"✅ Embedding model loaded! Dimension: {embedding_dimension}\")\nprint(f\"✅ LangChain text splitter initialized:\")\nprint(f\"  - Chunk size: {CONFIG['chunk_size']} characters\")\nprint(f\"  - Overlap: {CONFIG['chunk_overlap']} characters\")\nprint(f\"  - Separators: {CONFIG['separators']}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:05:16.994027Z","iopub.execute_input":"2025-08-01T13:05:16.994342Z","iopub.status.idle":"2025-08-01T13:05:25.283184Z","shell.execute_reply.started":"2025-08-01T13:05:16.994318Z","shell.execute_reply":"2025-08-01T13:05:25.282152Z"},"trusted":true},"outputs":[{"name":"stdout","text":"🔄 Loading embedding model: BAAI/bge-small-en\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d392611c5e247e785c6e86507ed9ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fbbfc88ec443e5980873dee0b53967"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563c43fc91ab4697ab5f5f267946e04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0248758560be4d68a18435afad018faf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d40e079f19470f8d6b8c51cdf4a0dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b1f4e1c2544712b9c8718d2003fe5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a21831bfe7b4487588b917db11a42c66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c9cd8bbbc64e0eb4e1faa13f4b13da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae69257bdd014df9b18c9a5118a364b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d289e3b885bd4775a9de5217f9e42ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518463378ad54fc4a7747d82d84b94e8"}},"metadata":{}},{"name":"stdout","text":"✅ Embedding model loaded! Dimension: 384\n✅ LangChain text splitter initialized:\n  - Chunk size: 300 characters\n  - Overlap: 50 characters\n  - Separators: ['\\n\\n', '\\n', ' ', '']\n","output_type":"stream"}],"execution_count":8},{"id":"b56ac3d3","cell_type":"code","source":"# Initialize Hugging Face text generation pipeline\nprint(f\"🔄 Loading generation model: {CONFIG['generation_model']}\")\n\ntry:\n    # Try GPU first\n    generator = pipeline(\n        \"text-generation\",\n        model=CONFIG['generation_model'],\n        device=0 if device == \"cuda\" else -1,\n        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n        max_length=1024,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.9,\n        pad_token_id=50256,  # Common padding token\n        trust_remote_code=True\n    )\n    print(\"✅ Text generation model loaded!\")\n    \nexcept Exception as e:\n    print(f\"⚠️ Trying simpler configuration: {e}\")\n    try:\n        generator = pipeline(\n            \"text-generation\",\n            model=CONFIG['generation_model'],\n            device=-1,  # Force CPU\n            max_length=512,\n            trust_remote_code=True\n        )\n        print(\"✅ Text generation model loaded on CPU!\")\n    except Exception as e2:\n        print(f\"❌ Model loading failed: {e2}\")\n        print(\"💡 Try a smaller model like 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\")\n        generator = None\n\n# Test the generator\nif generator:\n    test_prompt = \"Question: What is AI? Answer:\"\n    try:\n        test_output = generator(test_prompt, max_length=50, num_return_sequences=1)\n        print(f\"🧪 Generator test: {test_output[0]['generated_text'][len(test_prompt):].strip()[:50]}...\")\n    except:\n        print(\"⚠️ Generator test failed, but model is loaded\")","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:05:29.896279Z","iopub.execute_input":"2025-08-01T13:05:29.896582Z","iopub.status.idle":"2025-08-01T13:06:23.194231Z","shell.execute_reply.started":"2025-08-01T13:05:29.896560Z","shell.execute_reply":"2025-08-01T13:06:23.193342Z"},"trusted":true},"outputs":[{"name":"stdout","text":"🔄 Loading generation model: Qwen/Qwen2.5-3B-Instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806a79943de74257a2150107ff66b5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b65848a4cf4c8fa07260f80f1fa915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a06d03de98f5461684b52aee329693ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb0db52ccf94a84b900151778bd9ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894df1f88829478bbe855af94a0651e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae2063001544fae9dbc0d61a4a53e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c72eabcaa344aab34984e978acacc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef55a52e956464699fab232df9a036d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3832cbff13814d6bb2c280f0142580f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d7503b88d84e728bf27064ad3f346c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ab8b85aab8468984b17899a1f3b276"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"✅ Text generation model loaded!\n🧪 Generator test: Artificial intelligence (AI) is the simulation of ...\n","output_type":"stream"}],"execution_count":9},{"id":"da0960bd","cell_type":"markdown","source":"pinecone","metadata":{}},{"id":"77613373","cell_type":"code","source":"# Initialize Pinecone\nprint(\"🔄 Initializing Pinecone...\")\npc = Pinecone(api_key=PINECONE_API_KEY)\n\n# Check if index exists\nexisting_indexes = [idx.name for idx in pc.list_indexes()]\n\nif CONFIG['index_name'] not in existing_indexes:\n    print(f\"🔄 Creating Pinecone index: {CONFIG['index_name']}\")\n    pc.create_index(\n        name=CONFIG['index_name'],\n        dimension=embedding_dimension,\n        metric=\"cosine\",\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n    )\n    print(\"⏳ Waiting for index to be ready...\")\n    time.sleep(10)\n\n# Connect to index\nindex = pc.Index(CONFIG['index_name'])\nprint(f\"✅ Connected to Pinecone index: {CONFIG['index_name']}\")\n\n# Initialize global variables for BM25\ndocuments_corpus = []\ndocument_metadata = []\nbm25_index = None\n\nprint(\"✅ All components initialized!\")","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:06:23.195656Z","iopub.execute_input":"2025-08-01T13:06:23.195978Z","iopub.status.idle":"2025-08-01T13:06:26.341511Z","shell.execute_reply.started":"2025-08-01T13:06:23.195951Z","shell.execute_reply":"2025-08-01T13:06:26.340544Z"},"trusted":true},"outputs":[{"name":"stdout","text":"🔄 Initializing Pinecone...\n✅ Connected to Pinecone index: hybrid-rag-langchain\n✅ All components initialized!\n","output_type":"stream"}],"execution_count":10},{"id":"d7b0d6c0","cell_type":"markdown","source":"# LangChain Document Chunking","metadata":{}},{"id":"cab5bd8f","cell_type":"code","source":"def chunk_documents(documents: List[str], doc_ids: Optional[List[str]] = None) -> List[Dict]:\n    \"\"\"\n    Chunk documents using LangChain RecursiveCharacterTextSplitter\n    \"\"\"\n    if doc_ids and len(doc_ids) != len(documents):\n        raise ValueError(\"doc_ids length must match documents length\")\n    \n    all_chunks = []\n    \n    for i, doc_text in enumerate(documents):\n        doc_id = doc_ids[i] if doc_ids else f\"doc_{i}\"\n        \n        # Create LangChain Document object\n        doc = Document(\n            page_content=doc_text,\n            metadata={\"doc_id\": doc_id, \"original_length\": len(doc_text)}\n        )\n        \n        # Split using LangChain text splitter\n        split_docs = text_splitter.split_documents([doc])\n        \n        # Convert to our format\n        for chunk_idx, split_doc in enumerate(split_docs):\n            chunk_text = split_doc.page_content\n            word_count = len(chunk_text.split())\n            char_count = len(chunk_text)\n            \n            all_chunks.append({\n                \"id\": f\"{doc_id}_chunk_{chunk_idx}\",\n                \"text\": chunk_text,\n                \"metadata\": {\n                    \"doc_id\": doc_id,\n                    \"chunk_idx\": chunk_idx,\n                    \"total_chunks\": len(split_docs),\n                    \"word_count\": word_count,\n                    \"char_count\": char_count,\n                    \"original_length\": len(doc_text)\n                }\n            })\n    \n    # Calculate statistics\n    total_chunks = len(all_chunks)\n    avg_words = sum(c['metadata']['word_count'] for c in all_chunks) / total_chunks if total_chunks else 0\n    avg_chars = sum(c['metadata']['char_count'] for c in all_chunks) / total_chunks if total_chunks else 0\n    \n    print(f\"✅ Created {total_chunks} LangChain chunks from {len(documents)} documents\")\n    print(f\"📊 Average per chunk: {avg_words:.1f} words, {avg_chars:.0f} characters\")\n    \n    return all_chunks\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:06:41.225225Z","iopub.execute_input":"2025-08-01T13:06:41.225575Z","iopub.status.idle":"2025-08-01T13:06:41.234381Z","shell.execute_reply.started":"2025-08-01T13:06:41.225550Z","shell.execute_reply":"2025-08-01T13:06:41.233475Z"},"trusted":true},"outputs":[],"execution_count":12},{"id":"1406ef18","cell_type":"markdown","source":"embedding generation","metadata":{}},{"id":"bee21229","cell_type":"code","source":"def vectorize_chunks(chunks: List[Dict]) -> List[Dict]:\n    \"\"\"Generate embeddings for chunks using BAAI/bge-small-en\"\"\"\n    if not chunks:\n        return []\n    \n    print(f\"🔄 Generating embeddings for {len(chunks)} chunks...\")\n    \n    # Extract texts\n    texts = [chunk[\"text\"] for chunk in chunks]\n    \n    # Generate embeddings in batch\n    embeddings = embedding_model.encode(\n        texts,\n        show_progress_bar=True,\n        convert_to_numpy=True,\n        normalize_embeddings=True  # Important for cosine similarity\n    )\n    \n    # Add embeddings to chunks\n    for chunk, embedding in zip(chunks, embeddings):\n        chunk[\"embedding\"] = embedding.tolist()\n    \n    print(\"✅ Embeddings generated successfully\")\n    return chunks\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:06:44.661878Z","iopub.execute_input":"2025-08-01T13:06:44.662213Z","iopub.status.idle":"2025-08-01T13:06:44.667595Z","shell.execute_reply.started":"2025-08-01T13:06:44.662188Z","shell.execute_reply":"2025-08-01T13:06:44.666734Z"},"trusted":true},"outputs":[],"execution_count":13},{"id":"a5797b07","cell_type":"markdown","source":"storing in pinecone","metadata":{}},{"id":"70fd230f","cell_type":"code","source":"def store_in_pinecone(chunks: List[Dict]) -> bool:\n    \"\"\"Store chunks in Pinecone vector database\"\"\"\n    if not chunks:\n        return False\n    \n    try:\n        print(f\"🔄 Storing {len(chunks)} chunks in Pinecone...\")\n        \n        vectors = []\n        for chunk in chunks:\n            vector = {\n                \"id\": chunk[\"id\"],\n                \"values\": chunk[\"embedding\"],\n                \"metadata\": {\n                    \"text\": chunk[\"text\"][:1000],  # Pinecone metadata limit\n                    \"doc_id\": chunk[\"metadata\"][\"doc_id\"],\n                    \"chunk_idx\": chunk[\"metadata\"][\"chunk_idx\"],\n                    \"word_count\": chunk[\"metadata\"][\"word_count\"],\n                    \"char_count\": chunk[\"metadata\"][\"char_count\"]\n                }\n            }\n            vectors.append(vector)\n        \n        # Batch upsert\n        batch_size = 100\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            index.upsert(vectors=batch)\n        \n        print(\"✅ Successfully stored in Pinecone\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ Failed to store in Pinecone: {e}\")\n        return False\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:06:46.749344Z","iopub.execute_input":"2025-08-01T13:06:46.749988Z","iopub.status.idle":"2025-08-01T13:06:46.756427Z","shell.execute_reply.started":"2025-08-01T13:06:46.749950Z","shell.execute_reply":"2025-08-01T13:06:46.755398Z"},"trusted":true},"outputs":[],"execution_count":14},{"id":"76ba02f9","cell_type":"markdown","source":"BM25 indexing\n","metadata":{}},{"id":"aace06fb","cell_type":"code","source":"def build_bm25_index(chunks: List[Dict]) -> bool:\n    \"\"\"Build BM25 index for keyword-based retrieval\"\"\"\n    global documents_corpus, document_metadata, bm25_index\n    \n    try:\n        print(\"🔄 Building BM25 index...\")\n        \n        # Store documents and metadata\n        documents_corpus = [chunk[\"text\"] for chunk in chunks]\n        document_metadata = [\n            {\n                \"id\": chunk[\"id\"],\n                \"doc_id\": chunk[\"metadata\"][\"doc_id\"],\n                \"chunk_idx\": chunk[\"metadata\"][\"chunk_idx\"]\n            }\n            for chunk in chunks\n        ]\n        \n        # Tokenize documents for BM25\n        tokenized_docs = [doc.lower().split() for doc in documents_corpus]\n        \n        # Build BM25 index\n        bm25_index = BM25Okapi(tokenized_docs)\n        \n        print(f\"✅ BM25 index built with {len(documents_corpus)} documents\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ Failed to build BM25 index: {e}\")\n        return False\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:06:48.923982Z","iopub.execute_input":"2025-08-01T13:06:48.924540Z","iopub.status.idle":"2025-08-01T13:06:48.930466Z","shell.execute_reply.started":"2025-08-01T13:06:48.924517Z","shell.execute_reply":"2025-08-01T13:06:48.929789Z"},"trusted":true},"outputs":[],"execution_count":15},{"id":"02317391","cell_type":"markdown","source":"Semantic search from the vectorDB","metadata":{}},{"id":"cebeedfa","cell_type":"code","source":"def semantic_search(query: str, top_k: int = 10) -> List[Dict]:\n    \"\"\"Semantic search using Pinecone\"\"\"\n    try:\n        # Generate query embedding\n        query_embedding = embedding_model.encode([query], normalize_embeddings=True)[0]\n        \n        # Search Pinecone\n        results = index.query(\n            vector=query_embedding.tolist(),\n            top_k=top_k,\n            include_metadata=True\n        )\n        \n        # Format results\n        semantic_results = []\n        for match in results.matches:\n            semantic_results.append({\n                \"id\": match.id,\n                \"score\": match.score,\n                \"text\": match.metadata.get(\"text\", \"\"),\n                \"doc_id\": match.metadata.get(\"doc_id\", \"\"),\n                \"source\": \"semantic\"\n            })\n        \n        return semantic_results\n        \n    except Exception as e:\n        print(f\"❌ Semantic search failed: {e}\")\n        return []\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:02.776351Z","iopub.execute_input":"2025-08-01T13:07:02.776976Z","iopub.status.idle":"2025-08-01T13:07:02.782704Z","shell.execute_reply.started":"2025-08-01T13:07:02.776909Z","shell.execute_reply":"2025-08-01T13:07:02.781972Z"},"trusted":true},"outputs":[],"execution_count":17},{"id":"50775feb","cell_type":"markdown","source":"keyword search via BM25 \n","metadata":{}},{"id":"51a5934f","cell_type":"code","source":"def keyword_search(query: str, top_k: int = 10) -> List[Dict]:\n    \"\"\"Keyword-based search using BM25\"\"\"\n    if bm25_index is None:\n        print(\"⚠️ BM25 index not built\")\n        return []\n    \n    try:\n        # Tokenize query\n        query_tokens = query.lower().split()\n        \n        # Get BM25 scores\n        scores = bm25_index.get_scores(query_tokens)\n        \n        # Get top-k results\n        top_indices = np.argsort(scores)[::-1][:top_k]\n        \n        # Format results\n        keyword_results = []\n        for idx in top_indices:\n            if scores[idx] > 0:  # Only include positive scores\n                keyword_results.append({\n                    \"id\": document_metadata[idx][\"id\"],\n                    \"score\": float(scores[idx]),\n                    \"text\": documents_corpus[idx],\n                    \"doc_id\": document_metadata[idx][\"doc_id\"],\n                    \"source\": \"keyword\"\n                })\n        \n        return keyword_results\n        \n    except Exception as e:\n        print(f\"❌ Keyword search failed: {e}\")\n        return []\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:05.623387Z","iopub.execute_input":"2025-08-01T13:07:05.624281Z","iopub.status.idle":"2025-08-01T13:07:05.633008Z","shell.execute_reply.started":"2025-08-01T13:07:05.624243Z","shell.execute_reply":"2025-08-01T13:07:05.631965Z"},"trusted":true},"outputs":[],"execution_count":18},{"id":"7d02f6e9","cell_type":"markdown","source":"RRF to combine bm25 and semantic","metadata":{}},{"id":"869ad9f7","cell_type":"code","source":"def reciprocal_rank_fusion(semantic_results: List[Dict], keyword_results: List[Dict], k: int = 60) -> List[Dict]:\n    \"\"\"Combine semantic and keyword results using Reciprocal Rank Fusion\"\"\"\n    \n    # Create score dictionaries\n    rrf_scores = {}\n    \n    # Add semantic search scores\n    for rank, result in enumerate(semantic_results):\n        doc_id = result[\"id\"]\n        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k + rank + 1)\n    \n    # Add keyword search scores\n    for rank, result in enumerate(keyword_results):\n        doc_id = result[\"id\"]\n        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k + rank + 1)\n    \n    # Create combined results\n    all_results = {}\n    \n    # Add all results to dictionary\n    for result in semantic_results + keyword_results:\n        doc_id = result[\"id\"]\n        if doc_id not in all_results:\n            all_results[doc_id] = result.copy()\n            all_results[doc_id][\"rrf_score\"] = rrf_scores[doc_id]\n            all_results[doc_id][\"sources\"] = [result[\"source\"]]\n        else:\n            if result[\"source\"] not in all_results[doc_id][\"sources\"]:\n                all_results[doc_id][\"sources\"].append(result[\"source\"])\n    \n    # Sort by RRF score\n    fused_results = list(all_results.values())\n    fused_results.sort(key=lambda x: x[\"rrf_score\"], reverse=True)\n    \n    return fused_results\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:07.728781Z","iopub.execute_input":"2025-08-01T13:07:07.729622Z","iopub.status.idle":"2025-08-01T13:07:07.736750Z","shell.execute_reply.started":"2025-08-01T13:07:07.729591Z","shell.execute_reply":"2025-08-01T13:07:07.735968Z"},"trusted":true},"outputs":[],"execution_count":19},{"id":"b04d4d5c","cell_type":"markdown","source":"hugginface respone generation","metadata":{}},{"id":"2bfcb1e3","cell_type":"code","source":"def generate_response_hf(query: str, context_docs: List[Dict], max_new_tokens: int = 800) -> str:\n    \"\"\"Generate response using Hugging Face model with retrieved context\"\"\"\n    if not context_docs:\n        return \"I couldn't find relevant information to answer your question.\"\n    \n    if generator is None:\n        return \"Text generation model not available. Please check model loading.\"\n    \n    # Prepare context (increased limits)\n    context_parts = []\n    for i, doc in enumerate(context_docs[:5]):  # Use top 5 docs\n        context_parts.append(f\"Context {i+1}: {doc['text'][:800]}\")  # 800 chars per doc\n    \n    context = \"\\n\".join(context_parts)\n    \n    # Clean, simple prompt that won't confuse the model\n    prompt = f\"\"\"Context: {context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n    \n    try:\n        # Generate response\n        response = generator(\n            prompt,\n            max_new_tokens=max_new_tokens,\n            num_return_sequences=1,\n            truncation=True,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=generator.tokenizer.eos_token_id if generator.tokenizer.eos_token_id else 50256\n        )\n        \n        # Extract generated text\n        generated_text = response[0]['generated_text']\n        \n        # Extract only the answer part (after \"Answer:\")\n        if \"Answer:\" in generated_text:\n            answer = generated_text.split(\"Answer:\")[-1].strip()\n        else:\n            answer = generated_text[len(prompt):].strip()\n        \n        # Clean up the answer - remove any extra instructions\n        if \"You are an AI assistant\" in answer:\n            answer = answer.split(\"You are an AI assistant\")[0].strip()\n        if \"Task:\" in answer:\n            answer = answer.split(\"Task:\")[0].strip()\n        \n        # Allow longer responses (up to 10 lines)\n        answer_lines = answer.split('\\n')\n        answer = '\\n'.join(answer_lines[:10]) if len(answer_lines) > 1 else answer\n        \n        # Ensure minimum length\n        if not answer or len(answer) < 20:\n            return \"I need more context to provide a comprehensive answer.\"\n        \n        return answer\n        \n    except Exception as e:\n        print(f\"❌ Generation failed: {e}\")\n        return \"Sorry, I encountered an error while generating the response.\"","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:11.144826Z","iopub.execute_input":"2025-08-01T13:07:11.145150Z","iopub.status.idle":"2025-08-01T13:07:11.153252Z","shell.execute_reply.started":"2025-08-01T13:07:11.145129Z","shell.execute_reply":"2025-08-01T13:07:11.152268Z"},"trusted":true},"outputs":[],"execution_count":20},{"id":"3b406fc1","cell_type":"markdown","source":"evaluation using cosine similarity","metadata":{}},{"id":"50d6808c","cell_type":"code","source":"def evaluate_response_simple(query: str, response: str, context_docs: List[Dict]) -> Dict:\n    \"\"\"Simple evaluation without generating follow-up questions\"\"\"\n    \n    # Simple metrics\n    response_length = len(response.split())\n    context_used = len(context_docs)\n    \n    # Check if response contains key terms from query\n    query_words = set(query.lower().split())\n    response_words = set(response.lower().split())\n    word_overlap = len(query_words.intersection(response_words)) / len(query_words) if query_words else 0\n    \n    # Semantic similarity between query and response\n    try:\n        query_embedding = embedding_model.encode([query])\n        response_embedding = embedding_model.encode([response])\n        semantic_similarity = cosine_similarity(query_embedding, response_embedding)[0][0]\n    except:\n        semantic_similarity = 0.0\n    \n    # Context relevance (average similarity between query and context docs)\n    context_relevance = 0.0\n    if context_docs:\n        try:\n            context_texts = [doc['text'] for doc in context_docs[:3]]\n            context_embeddings = embedding_model.encode(context_texts)\n            query_embedding = embedding_model.encode([query])\n            relevance_scores = cosine_similarity(query_embedding, context_embeddings)[0]\n            context_relevance = np.mean(relevance_scores)\n        except:\n            context_relevance = 0.0\n    \n    return {\n        \"query\": query,\n        \"response\": response,\n        \"response_length\": response_length,\n        \"context_docs_used\": context_used,\n        \"word_overlap_score\": word_overlap,\n        \"semantic_similarity\": float(semantic_similarity),\n        \"context_relevance\": float(context_relevance),\n        \"overall_score\": (word_overlap + semantic_similarity + context_relevance) / 3\n    }\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:14.266212Z","iopub.execute_input":"2025-08-01T13:07:14.266832Z","iopub.status.idle":"2025-08-01T13:07:14.274370Z","shell.execute_reply.started":"2025-08-01T13:07:14.266804Z","shell.execute_reply":"2025-08-01T13:07:14.273298Z"},"trusted":true},"outputs":[],"execution_count":21},{"id":"fbb190f2","cell_type":"markdown","source":"Pipeline\n","metadata":{}},{"id":"5bb08c8f","cell_type":"code","source":"def rag_chat_flexible(query: str, method: str = \"hybrid\", top_k: int = 10) -> Dict:\n    \"\"\"\n    Flexible RAG pipeline - supports any retrieval method\n    \n    Args:\n        query: Question to ask\n        method: \"semantic\", \"keyword\", \"hybrid\" (default: \"hybrid\")\n        top_k: Number of chunks to retrieve\n    \"\"\"\n    print(f\"🔄 Processing query with {method.upper()} retrieval: {query}\")\n    \n    try:\n        # Route to appropriate retrieval method\n        if method == \"semantic\":\n            retrieved_docs = semantic_search(query, top_k)\n        elif method == \"keyword\":\n            retrieved_docs = keyword_search(query, top_k)\n        elif method == \"hybrid\":\n            sem_results = semantic_search(query, top_k)\n            key_results = keyword_search(query, top_k)\n            retrieved_docs = reciprocal_rank_fusion(sem_results, key_results)\n        else:\n            raise ValueError(f\"Unknown method: {method}. Use 'semantic', 'keyword', or 'hybrid'\")\n        \n        # Generate response\n        response = generate_response_hf(query, retrieved_docs)\n        evaluation = evaluate_response_simple(query, response, retrieved_docs)\n        \n        return {\n            \"query\": query,\n            \"answer\": response,\n            \"retrieval_method\": method,\n            \"num_docs_retrieved\": len(retrieved_docs),\n            \"retrieved_docs\": retrieved_docs[:3],\n            \"doc_sources\": list(set([doc[\"doc_id\"] for doc in retrieved_docs])),\n            \"evaluation\": evaluation\n        }\n        \n    except Exception as e:\n        print(f\"❌ Chat pipeline failed: {e}\")\n        return {\n            \"query\": query,\n            \"answer\": f\"Error: {e}\",\n            \"retrieval_method\": method,\n            \"num_docs_retrieved\": 0,\n            \"retrieved_docs\": [],\n            \"doc_sources\": [],\n            \"evaluation\": {}\n        }\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:17.286596Z","iopub.execute_input":"2025-08-01T13:07:17.287750Z","iopub.status.idle":"2025-08-01T13:07:17.295150Z","shell.execute_reply.started":"2025-08-01T13:07:17.287716Z","shell.execute_reply":"2025-08-01T13:07:17.294293Z"},"trusted":true},"outputs":[],"execution_count":22},{"id":"b0f4654d","cell_type":"markdown","source":"testing using docs","metadata":{}},{"id":"a1f8d590","cell_type":"code","source":"def add_documents_to_system(documents: List[str], doc_ids: Optional[List[str]] = None) -> bool:\n    \"\"\"\n    Complete pipeline to add documents using LangChain text splitter\n    \"\"\"\n    try:\n        print(f\"📄 Adding {len(documents)} documents with LangChain chunking...\")\n        \n        # Step 1: LangChain chunking\n        chunks = chunk_documents(documents, doc_ids)\n        \n        if not chunks:\n            print(\"❌ No chunks created\")\n            return False\n        \n        # Step 2: Generate embeddings\n        chunks_with_embeddings = vectorize_chunks(chunks)\n        \n        # Step 3: Store in Pinecone\n        pinecone_success = store_in_pinecone(chunks_with_embeddings)\n        \n        # Step 4: Build BM25 index for keyword search\n        bm25_success = build_bm25_index(chunks_with_embeddings)\n        \n        success = pinecone_success and bm25_success\n        \n        if success:\n            print(\"✅ Successfully added all documents with LangChain chunking\")\n            print(f\"📊 Total chunks in system: {len(chunks_with_embeddings)}\")\n            \n            # Show chunking statistics\n            total_words = sum(c['metadata']['word_count'] for c in chunks_with_embeddings)\n            total_chars = sum(c['metadata']['char_count'] for c in chunks_with_embeddings)\n            avg_words = total_words / len(chunks_with_embeddings)\n            avg_chars = total_chars / len(chunks_with_embeddings)\n            \n            print(f\"📊 Chunking stats: {avg_words:.1f} avg words, {avg_chars:.0f} avg chars per chunk\")\n        else:\n            print(\"❌ Failed to add some documents\")\n        \n        return success\n        \n    except Exception as e:\n        print(f\"❌ Error adding documents: {e}\")\n        return False\n\n# Load your parsed text files\ndef load_extracted_text_files(directory_path: str = \"extracted_texts\") -> List[str]:\n    \"\"\"Load text files from your document processing pipeline\"\"\"\n    from pathlib import Path\n    \n    texts_dir = Path(directory_path)\n    if not texts_dir.exists():\n        print(f\"❌ Directory not found: {directory_path}\")\n        return []\n    \n    # Find all extracted text files\n    text_files = list(texts_dir.glob(\"*_extracted_text.txt\"))\n    \n    documents = []\n    doc_ids = []\n    \n    for file_path in text_files:\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read().strip()\n            \n            if content:  # Only add non-empty files\n                # Extract document name from filename\n                doc_name = file_path.stem.replace(\"_extracted_text\", \"\")\n                \n                documents.append(content)\n                doc_ids.append(doc_name)\n                \n        except Exception as e:\n            print(f\"⚠️ Error reading {file_path}: {e}\")\n    \n    print(f\"📄 Loaded {len(documents)} text files from {directory_path}\")\n    if documents:\n        total_words = sum(len(doc.split()) for doc in documents)\n        avg_words = total_words / len(documents)\n        print(f\"📊 Average document length: {avg_words:.1f} words\")\n    \n    return documents, doc_ids\n\n# Example usage - load and add your documents\ntry:\n    # Load your extracted text files\n    my_documents, my_doc_ids = load_extracted_text_files(\"/kaggle/input/dataset1\")\n    \n    if my_documents:\n        # Add to RAG system with word-based chunking\n        success = add_documents_to_system(my_documents, my_doc_ids)\n        \n        if success:\n            print(\"🎉 Ready to chat with your documents!\")\n        else:\n            print(\"❌ Setup failed\")\n    else:\n        print(\"⚠️ No documents found. Please check the 'extracted_texts' directory.\")\n        \nexcept Exception as e:\n    print(f\"❌ Error: {e}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-01T13:07:20.467840Z","iopub.execute_input":"2025-08-01T13:07:20.468143Z","iopub.status.idle":"2025-08-01T13:07:21.375479Z","shell.execute_reply.started":"2025-08-01T13:07:20.468120Z","shell.execute_reply":"2025-08-01T13:07:21.374785Z"},"trusted":true},"outputs":[{"name":"stdout","text":"📄 Loaded 1 text files from /kaggle/input/dataset1\n📊 Average document length: 1999.0 words\n📄 Adding 1 documents with LangChain chunking...\n✅ Created 61 LangChain chunks from 1 documents\n📊 Average per chunk: 36.4 words, 227 characters\n🔄 Generating embeddings for 61 chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8edf93fb95345febbf8097c67f2eaed"}},"metadata":{}},{"name":"stdout","text":"✅ Embeddings generated successfully\n🔄 Storing 61 chunks in Pinecone...\n✅ Successfully stored in Pinecone\n🔄 Building BM25 index...\n✅ BM25 index built with 61 documents\n✅ Successfully added all documents with LangChain chunking\n📊 Total chunks in system: 61\n📊 Chunking stats: 36.4 avg words, 227 avg chars per chunk\n🎉 Ready to chat with your documents!\n","output_type":"stream"}],"execution_count":23},{"id":"9ad3ab8e","cell_type":"code","source":"def ask_question(question: str):\n    \"\"\"Simple interface to ask questions about your aviation documents\"\"\"\n    print(f\"\\n🔍 Question: {question}\")\n    print(\"=\"*60)\n    \n    # Use hybrid retrieval (best performing method)\n    result = rag_chat_flexible(question)\n    \n    # Display results\n    print(f\"📋 Answer:\")\n    print(f\"   {result['answer']}\")\n    print(f\"\\n📄 Source Documents: {', '.join(result['doc_sources'])}\")\n    print(f\"📊 Confidence Score: {result['evaluation'].get('overall_score', 0):.3f}\")\n    print(f\"📈 Retrieved {result['num_docs_retrieved']} relevant chunks\")\n    \n    return None\n\n# Easy testing:\nask_question(\"Why was flight AC869 canceled?\")\nask_question(\"What GPS issues occurred?\")\nask_question(\"What maintenance was required?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:21:25.500874Z","iopub.execute_input":"2025-08-01T13:21:25.501701Z","iopub.status.idle":"2025-08-01T13:21:53.898604Z","shell.execute_reply.started":"2025-08-01T13:21:25.501675Z","shell.execute_reply":"2025-08-01T13:21:53.897719Z"}},"outputs":[{"name":"stdout","text":"\n🔍 Question: Why was flight AC869 canceled?\n============================================================\n🔄 Processing query with HYBRID retrieval: Why was flight AC869 canceled?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a9c81b38444fada3b828523943cf34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bcc220f6bc0441fa2880ccfa95f3113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e29bc051f24faa8160c8eea39e3aad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6703aa84f2c14ec1965eaa3ce624fa9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0fd9aa8a6ed49ee80ccf7eea95294e7"}},"metadata":{}},{"name":"stdout","text":"📋 Answer:\n   Flight AC869 was canceled due to extraordinary circumstances that prevented it from operating as planned. Specifically, the aircraft arrived at YHZ (likely an airport code for Toronto Pearson International Airport) with a Maintenance Engineering Limitation (MEL), which means there were specific issues or limitations with the aircraft's systems that required maintenance. This MEL prevented the aircraft from being made available for operation in London Heathrow as scheduled. Despite these challenges, efforts were made to protect passengers by rerouting them to other flights within existing capacity and providing an upgrade to ensure they were not stranded for more than eight hours. To summarize, the cancellation of flight AC869 was primarily due to unresolved maintenance issues affecting the aircraft's operational readiness.\n\n📄 Source Documents: claims_686be326f6267c89ac0cad27_686be326f6267c89ac0cad27_evidence\n📊 Confidence Score: 0.792\n📈 Retrieved 16 relevant chunks\n\n🔍 Question: What GPS issues occurred?\n============================================================\n🔄 Processing query with HYBRID retrieval: What GPS issues occurred?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0627dc18ed264b6c8589344e244cf88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3581b8c884134370a1173e0ec91d7de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad2bf2e128134ce59fd87800a0d725d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9526eb73a6ee46c5b3dcbf01a5eacc75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e160c09f990549e4be64af36526ee573"}},"metadata":{}},{"name":"stdout","text":"📋 Answer:\n   Based on the context provided, there were several GPS-related issues that occurred:\n\n1. **Industry-wide GPS Outage**: The text in both Context 2 and Context 3 mentions \"GPS OUTAGE\" at 58.7 PMMIMA (presumably May 22, 2025). This indicates a widespread GPS failure affecting air Canada and possibly other airlines as it is described as an \"industry-wide\" issue.\n\n📄 Source Documents: claims_686be326f6267c89ac0cad27_686be326f6267c89ac0cad27_evidence\n📊 Confidence Score: 0.759\n📈 Retrieved 16 relevant chunks\n\n🔍 Question: What maintenance was required?\n============================================================\n🔄 Processing query with HYBRID retrieval: What maintenance was required?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a185bf9e108a4c6ea81cd762f1356b72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37d4cb9c2db45e9828fc2f4a0077e89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d1bdd2677bc4b1fa2c161f728f40354"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4ab6ce637f42c2a66684794b8ad8ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e8563216537405188ddb71b16501eca"}},"metadata":{}},{"name":"stdout","text":"📋 Answer:\n   According to the context provided, when the pilots started up aircraft systems, a GPS 1 and 2 Fail would pop up, requiring Maintenance intervention to put on the MEL (Minimum Equipment List). This indicates that maintenance action was necessary due to the GPS issue.\n\nSo, the specific maintenance required was:\n\n📄 Source Documents: claims_686be326f6267c89ac0cad27_686be326f6267c89ac0cad27_evidence\n📊 Confidence Score: 0.740\n📈 Retrieved 15 relevant chunks\n","output_type":"stream"}],"execution_count":29},{"id":"d834fbd4","cell_type":"markdown","source":"comparing retrieval methods","metadata":{}},{"id":"44e10103","cell_type":"code","source":"\"\"\"\ndef compare_retrieval_methods(query: str, verbose: bool = True) -> Dict:\n    '''Compare all three retrieval methods for a single query'''\n    \n    methods = [\"semantic\", \"keyword\", \"hybrid\"]\n    results = {}\n    \n    for method in methods:\n        if verbose:\n            print(f\"\\n🔍 Testing {method.upper()} retrieval:\")\n        \n        # Get response using specific method\n        if method == \"semantic\":\n            sem_results = semantic_search(query, CONFIG['retrieval_k'])\n            retrieved_docs = sem_results\n        elif method == \"keyword\":\n            key_results = keyword_search(query, CONFIG['retrieval_k'])\n            retrieved_docs = key_results\n        elif method == \"hybrid\":\n            sem_results = semantic_search(query, CONFIG['retrieval_k'])\n            key_results = keyword_search(query, CONFIG['retrieval_k'])\n            retrieved_docs = reciprocal_rank_fusion(sem_results, key_results)\n        \n        # Generate response\n        response_text = generate_response_hf(query, retrieved_docs)\n        evaluation = evaluate_response_simple(query, response_text, retrieved_docs)\n        \n        response = {\n            \"query\": query,\n            \"answer\": response_text,\n            \"retrieval_method\": method,\n            \"num_docs_retrieved\": len(retrieved_docs),\n            \"retrieved_docs\": retrieved_docs[:3],\n            \"doc_sources\": list(set([doc[\"doc_id\"] for doc in retrieved_docs])),\n            \"evaluation\": evaluation\n        }\n        \n        # Store results\n        results[method] = {\n            \"response\": response,\n            \"score\": response[\"evaluation\"].get(\"overall_score\", 0)\n        }\n        \n        if verbose:\n            print(f\"  Answer: {response['answer'][:150]}...\")\n            print(f\"  Sources: {response['doc_sources']}\")\n            print(f\"  Score: {results[method]['score']:.3f}\")\n    \n    # Rank methods\n    ranked_methods = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)\n    \n    comparison_result = {\n        \"query\": query,\n        \"method_results\": results,\n        \"rankings\": ranked_methods,\n        \"best_method\": ranked_methods[0][0],\n        \"best_score\": ranked_methods[0][1]['score']\n    }\n    \n    if verbose:\n        print(f\"\\n🏆 BEST METHOD: {comparison_result['best_method'].upper()} (Score: {comparison_result['best_score']:.3f})\")\n    \n    return comparison_result\n\n# Example usage (uncomment to test):\n# comparison = compare_retrieval_methods(\"Why was flight AC869 canceled?\")\n\"\"\"\n","metadata":{"execution":{"iopub.execute_input":"2025-08-01T11:41:37.694931Z","iopub.status.busy":"2025-08-01T11:41:37.694656Z","iopub.status.idle":"2025-08-01T11:41:37.701689Z","shell.execute_reply":"2025-08-01T11:41:37.700933Z","shell.execute_reply.started":"2025-08-01T11:41:37.694909Z"},"trusted":true},"outputs":[],"execution_count":null}]}